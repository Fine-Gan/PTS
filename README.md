# PTS
PTS: A Three-Dimensional Joint Optimization Method for Accelerating Multi-Job Distributed DNN Training
# Abstract
In machine learning (ML) clusters where multiple distributed deep neural network (DNN) training jobs share network links, communication contention between jobs significantly degrades overall GPU compute utilization. The inherent time-variability of inter job communication interleaving renders existing passive inter-job scheduling methods ineffective. We present PTS to address this challenge. By jointly employing time-shift scheduling, congestion control, and gradient compression methods, PTS proactively shape communication patterns of different jobs to ensuring their communication phases to be compactly arranged, minimizing link congestion and idle periods. We build a model for the inter-job scheduling problem that finds a series of optimization configuration for time-shift scheduling, congestion control, and gradient compression. We further propose a dual-synchronous adaptive algorithm to ensure system scalability and stability under dynamic network. For training six representative DNN models on a real GPU cluster, PTS outperforms Cassini in both efficiency (up to 29% speedup in average iteration time) and stability (up to 58% reduction in 99th percentile tail iteration time).
# Purpose
The purpose of this project is to provide relevant implementation code from the paper [PTS: A Three-Dimensional Joint Optimization Method for Accelerating Multi-Job Distributed DNN Training].
# Status
The paper is currently under review and the source code will be made available when the paper is accepted.
